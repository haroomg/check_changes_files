{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import duckdb as dk\n",
    "from slugify import slugify\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import os\n",
    "from typing import Union\n",
    "from pprint import pprint as pp\n",
    "import chardet\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path: str, change_columns: bool = True, modify_file: bool = False) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"convierte un csv a un DataFrame, sin immportar su encoding o sep\"\"\"\n",
    "\n",
    "    # error, file does not exist\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise ValueError(f\"El archivo ingresado no existe o esta mal escrito.\")\n",
    "\n",
    "\n",
    "    # detectamos el encoding del csv\n",
    "    with open(file_path, 'rb') as archivo:\n",
    "            resultado = chardet.detect(archivo.read())\n",
    "            encoding = resultado['encoding']\n",
    "\n",
    "\n",
    "    # abrimos el archivo como lectura y cargamos sus valore en una variable\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "\n",
    "        dialect = csv.Sniffer().sniff(file.read(4096))\n",
    "\n",
    "        # reiniciamos la posicion en la que se esta apuntando en el archivo\n",
    "        file.seek(0)\n",
    "\n",
    "        delimiter =  dialect.delimiter\n",
    "        csv_reader = csv.reader(file, delimiter=delimiter)\n",
    "        data = list(csv_reader)\n",
    "\n",
    "    columns = data[0]\n",
    "    new_columns = []\n",
    "\n",
    "    # acomodamos los nombre de las columnas y cambiamos las columnas que salgan repetidas\n",
    "    if change_columns:\n",
    "\n",
    "        for column in columns:\n",
    "            name = slugify(column, separator=\"_\")\n",
    "\n",
    "            if name not in new_columns:\n",
    "                new_columns.append(name)\n",
    "            else:\n",
    "                i = 0\n",
    "                while True:\n",
    "                    new_name = f\"{name}_{i}\"\n",
    "                    if new_name not in new_columns:\n",
    "                        new_columns.append(new_name)\n",
    "                        break\n",
    "                    i+=1\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data[1:], columns= new_columns)\n",
    "\n",
    "    if modify_file:\n",
    "        df.to_csv(file_path, sep=\",\", encoding=\"utf8\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_hash(texto):\n",
    "    # Crear un objeto hash SHA256\n",
    "    sha256_hash = hashlib.sha256()\n",
    "\n",
    "    # Convertir el texto en bytes y actualizar el hash\n",
    "    sha256_hash.update(texto.encode('utf-8'))\n",
    "\n",
    "    # Obtener el hash en formato hexadecimal\n",
    "    hash_resultado = sha256_hash.hexdigest()\n",
    "\n",
    "    return hash_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_json(report, path):\n",
    "    with open(path, 'w', encoding=\"utf8\") as file:\n",
    "        json.dump(report, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_changes(*args: str, get_report: bool = False) -> dict:\n",
    "    \n",
    "    legend: dict = {str(i):args[i] for i in range(len(args))}\n",
    "    report = {}   \n",
    "    \n",
    "    # lista con los skus en los cuales se detecto un cambio\n",
    "    changes_detected: dict = {}\n",
    "    \n",
    "    df = read_csv(args[0])\n",
    "    \n",
    "    for i in range(1,len(args)):\n",
    "    \n",
    "        # df = df_list[i-1]\n",
    "        df2 = read_csv(args[i])\n",
    "        \n",
    "        before = str(i-1)\n",
    "        after = str(i)\n",
    "        \n",
    "        # columns del df\n",
    "        df_columns = df.columns\n",
    "        \n",
    "        # lista con los sku de la data\n",
    "        sku_df = [sku[0] for sku in dk.sql(\"select distinct sku from df\").fetchall()]\n",
    "\n",
    "        # sku que no aparecen en la data\n",
    "        # sku_not_in_df = [sku[0] for sku in dk.sql(\"select distinct sku from df2 where sku not in (select distinct sku from df)\").fetchall()]\n",
    "\n",
    "        for sku in sku_df:\n",
    "            \n",
    "            data = dk.sql(f\"select * from df where sku = '{sku}'\").df().iloc[0].to_dict()\n",
    "            previous_date = dk.sql(f\"select * from df2 where sku = '{sku}'\").df().iloc[0].to_dict()\n",
    "            \n",
    "            for col in df_columns:\n",
    "                \n",
    "                # hasheamos los strings para comparar si son diferentes\n",
    "                data_hash = generar_hash(data[col])\n",
    "                previous_date_hash = generar_hash(previous_date[col])\n",
    "                \n",
    "                if data_hash != previous_date_hash:\n",
    "                    \n",
    "                    if sku not in changes_detected:\n",
    "                        changes_detected[sku] = {}\n",
    "                    \n",
    "                    if col not in changes_detected[sku]:\n",
    "                        \n",
    "                        changes_detected[sku][col] = {\n",
    "                                    before: data[col] ,\n",
    "                                    after: previous_date[col]\n",
    "                                }\n",
    "                    else:\n",
    "                        changes_detected[sku][col][before] = data[col] \n",
    "                        changes_detected[sku][col][after] = previous_date[col]\n",
    "        \n",
    "        df = df2\n",
    "    \n",
    "    report = {\n",
    "        \"changes detected\" : changes_detected if len(changes_detected) else None,\n",
    "        # \"sku are not there\" : sku_not_in_df if len(sku_not_in_df) else None,\n",
    "        \"legend\": legend if len(changes_detected) else None,\n",
    "    } \n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = check_changes(\"data/Amazon_Contents_17_10_2023.csv\", \"data/Amazon_Contents_18_10_2023.csv\", \"data/Amazon_Contents_19_10_2023.csv\", \"data/Amazon_Contents_20_10_2023.csv\", \"data/Amazon_Contents_21_10_2023.csv\")\n",
    "path_file = \"test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_json(data, path_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
